Speaker,Time,Date,Duration,Statement,Region,High Contracting Party
UNITED STATES,15:21:08,2024-03-06,0:00:20,"Thank you for breaking the ice . With apologies , if it is okay , we would return to topic 2 and do an intervention on that . So apologies for not breaking the ice on chapter 3 . But with your indulgence , before that conversation started , we were hoping just to come back on question 2 , topic 2 , if we may .",Western European and other States,Yes
UNITED STATES,15:21:52,2024-03-06,0:12:25,"So this is still for the list on topic 2 . You have the floor . Thank you . Thank you much , Mr. Chair . So just some reactions just with regard to the specific text and bullets and really appreciating the very good discussion we have had so far on it . So just with regard to the chapeau language about the legality of laws depends on , I think our suggestion would be to really flesh this out a little bit and instead refer to the legality of the use of laws to conduct a tax . And we see this as consistent with our general remarks that what we are doing in this question is addressing the use and the legality of the use of laws . And then we think it would be useful to refer to a tax in line with the specific IHL rules that are applicable to a tax that the group has found consensus on , distinction , proportionality , precautions in a tax . So wanted to provide a few concrete text ideas about how we can focus and take our discussion forward . Just with regard to the the first bullet , use and or functioning operational environment and nature of targets , I think as other Delegations have noted , these are factors that may be relevant and under legal requirements , they are practical and important considerations , but we do not see these as independent legal criteria or legal rules . A very similar comment with regard to the next bullet in a specific context , of course , we very much agree with the good discussion and many other Delegations that have raised the point that specific context is relevant to the application of IHL rules . However , I think we would note that we do not think it would be fruitful to try to elaborate different categories of context in which laws may be used or not used . With regard to the next bullet , human control involvement supervision , I think here we would note that we have already negotiated language on human machine interaction and a great deal of good consensus work has been done on that . As we stated before , the US Delegation does not support characterizing human control as a requirement of IHL . It is a means , it is one means , an important means , but not the only means that can enable compliance with IHL . Also see as a way out of some of this language , I wanted to appreciate the suggestion from Australian colleagues about the GGE already having reached consensus on the concept of operating systems within a responsible chain of human command and control . Of course , there is the language on control that I think you noted with regard to the 2023 GGE report . With regard to the next bullet , throughout the life cycle of laws , et cetera , I think the general comment we wanted to make here was a desire to move our conversation towards greater specificity and enabling more effective implementation of the various measures that will be a part of our instrument . We already have broad language about the importance of human machine interaction throughout the life cycle . Rather than renegotiating that guiding principle , we think we should be focusing on what requirements at particular stages of the life cycle can be taken or what practices can be taken at the various stages in support of the effective implementation of the legal requirements during use . Of course , use is where IHL applies to the conduct of hostilities , to the use of the weapons , but it is an important insight that the GGE has already reached that measures at various stages contribute to effective implementation and compliance with IHL during the use stage . In our draft articles proposal that we have put forward and other Delegations have supported , we articulate measures during development that can contribute to , for example , reducing the risk to civilians . With regard to the next bullet about the human operator beginning to articulate a number of subpoints about imposing limits , of course , as a general point , we want to recall that we do not see this bullet as a requirement of IHL specifically . This is not an IHL rule , but more practical consideration that helps implement IHL rules . We also wanted to make a factual point here that when we are talking about limitations regarding space and time or the types of targets and the duration of operation , geographic scope and scale of use , these will not necessarily be things that the operator does . How long the system operates or what types of targets it can engage , these are factors that will be potentially set during the development of the weapons system rather than things that the operator of the weapons system will be able to manipulate . So we did want to also bring that practical consideration into the conversation here . With regard to the next bullet , too , predict , trace and explain the behavior of laws , again , it does not reflect existing IHL and we support further work to try and explain how these concepts relate to specific IHL rules . For example , the principle of proportionality requires an expectation of the risks of civilian harm . That is that risk of civilian harm is what we are trying to predict , not abstractly the behavior of the weapons system . So wanting to evolve and focus our work on this particular bullet , the same comment , I think , with regard to the next bullet , we are talking generally about information . We want to progress our understanding and our measures and talk specifically about what kind of information is useful to conduct proportionality assessments to fulfill those IHL requirements , for example . And then lastly , with regard to changing the mode of the operation including deactivation , I just would recall the general point in our intervention that this could be usefully under the rubric of feasible precautions and that might be a way to conceptualize this particular practice and then also build out other important practices that could be part of our instrument . Lastly , we wanted to comment on this last bullet about reliance on transfunctions must be in good faith and consistent with due diligence . In particular , we wanted to appreciate the question from our ICRAC colleagues about this language about good faith and where it comes from . So really wanted to respond to that question . You know , for us , for the US , we believe that the law of war requires that all decision making in armed conflict in the implementation of IHL rules must be undertaken in good faith on the basis of the available information . And we see this as reflection of broader principle in customary international law that good faith performance of legal obligations is sort of foundational to the implementation of legal obligations . The principle of good faith is recognized in the Vienna Convention on the law of treaties . We believe that it also applies to the implementation of IHL and every member of the armed forces , you know , for example , in our view has a duty to comply with IHL in good faith . We would also recall that the GGE found consensus on this concept of good faith in 2019 . In the 2019 report , paragraph 17H , talking about how IHL compliance requires that human beings make certain judgments in good faith based on their assessment of the information available to them at the time . So we hope that is a useful explanation of where we see good faith and why we think it to be particularly relevant . And the other comment we would like to make with regard to this bullet is really just to connect it again to other IHL rules . And this is I think what we emphasized in our response to you , Mr. Chair , to this guiding question that the combatants' reliance on autonomous functions to identify , select or engage targets must be in good faith and in light of the information available at the time and must be consistent with due diligence in the implementation of the requirements and principles of distinction , proportionality and precautions in attack . And so we do want to connect the concept of good faith and due diligence to those specific IHL rules . And we think that is an important part of what we are trying to do here and this broader exercise of understanding how IHL applies to the use of autonomous weapon systems . And if you will bear with me , I will just try and explain it a little further . So from our perspective , IHL rules on the conduct of attacks like distinction apply to the commanders and operators , not the weapons . And that was a point we made earlier . It is not the weapon that was distinguished but the commander or the operator . With an autonomous weapon system , however , the commander and operator is relying on the weapon system to perform functions like selecting and engaging targets . And that reliance is I think what we are trying to highlight . IHL , of course , does not prohibit commanders and operators from using tools to assist them in combat . And so for us , an important legal question here is does the operator or commander's reliance on this tool , on the autonomous weapon system automating the target selection and engagement function , is that consistent with due diligence in the implementation of distinction ? Are they satisfying due diligence when relying on the weapon system to identify , select and engage the target ? And our draft article's proposal tries to specify some of the factors that are relevant to this analysis and we think this is an important area in which the GGE should do more work and could usefully do more work to strengthen the implementation of IHL . I thank you , Mr. Chair .",Western European and other States,Yes
AUSTRIA,15:34:45,2024-03-06,0:01:49,"We will give them the floor . Sir , you have the floor . On this topic again , we just wanted to react in the spirit of an interactive debate . It is mostly about the last point that was brought up about the good faith and due diligence . We believe it is an interesting topic and it is something that we could discuss , but in our understanding and this is maybe some of the questions that this raises , the implementation in good faith and due diligence is normally something that applies to states . We know it from outer space treaty , from experience I also know it from other ways how obligations of states could be implemented . If we push this and here again , I mean , in the shorter version it is not clear who should rely on this autonomous functions in good faith and consistent with due diligence . From how it is on the screen , it is not really clear who is the subject in this formulation . We would be hesitant to push these kind of obligations to commanders and soldiers in the field . We see this rather as a state obligation and it seems to be a general tendency in this topic to push we are doing it as well in our working paper . I am clearly aware of that . But we should not go beyond in pushing too much of these state obligations or responsibilities on the persons who actually have to handle these weapon systems . This could have a number of negative effects . It also could lead to military lack of effectiveness if the commanders or the operators will not use weapon systems because they are not clear how it works and how severe the obligations and consequences for them would be . But this is just a side note . Thank you . I would like to thank you for your reaction .",Western European and other States,Yes
AUSTRALIA,15:36:42,2024-03-06,0:06:33,"With your indulgence , Chair , we would also like to return to the second part of topic 2 . We would as a preliminary comment echo the point made by several other Delegations that is helpful to be clear about the task we are engaging in under topic 2 . We see this as an exercise in clarifying how existing prohibitions and limitations under IHL apply to laws . We see this as critical work that needs to be done before any new additional measures might be considered . Chair , in part 1 of topic 2 we spoke about laws that are inherently incapable of being used in compliance with IHL . We understand that here in part 2 of topic 2 we are focused on weapon systems that do not fall into this category . That is we are talking about weapon systems that are capable of being used in compliance with IHL . As has been noted , IHL imposes restrictions on the manner in which weapon systems are used . The cardinal principles of IHL are the principles of distinction , proportionality and precautions in attack . It is helpful to recall that the 2023 laws GGE report recognized that control with regard to laws is needed to uphold compliance with international law including IHL . In Australia's view , the types of control measures needed to ensure compliance with IHL in the use of laws will be context specific . The controls needed will depend on the capabilities and limitations of the weapon system , the nature of the targets and the operational environment . We also consider that an appropriate level of human involvement across the life cycle of a weapon system will be required to ensure compliance with IHL . The specific type of human machine interaction that might be required will be highly context specific . Responding now to a few specific elements in the text , as a general observation on the chapeau language , we do not believe the legality of the use of laws depends on all the factors listed here . We think it is important to differentiate requirements under IHL from additional practical measures that help implement IHL . We have a comment to make on bullet point 3 which refers to human control involvement and supervision . Here we would reiterate our position that the appropriate degree of human involvement is context dependent . Very different controls and levels of human involvement would be required in a static environment with no civilians or civilian objects present as opposed to a cluttered dynamic environment where there is a risk of civilian harm . Regarding bullet point 4 which refers to at various stages of the life cycle and throughout the life cycle , Australia favors a formulation of this bullet point that refers to relevant stages or various stages of the life cycle of the laws as is necessary to ensure that the use of laws is in compliance with IHL . Regarding the language further down which refers to predict , trace and explain the behavior of laws , we have a comment to make on the word predict . There are certainly questions regarding the level of predictability or reliability of a weapon system's effects that are required to ensure that a weapon system is capable of complying with IHL . While we recognize that IHL requires those who plan or decide upon an attack to anticipate certain effects , this may not necessarily require the operator being able to predict the behavior of laws in causing the anticipated effect . So we think this is an idea that needs some further consideration . Moving on to the notion that it is necessary to trace the behavior of laws , in our view this concept may be best addressed by only deploying and operating laws within a responsible chain of human command and control . We also think ideas about traceability that have been put forward by some Delegations could be helpfully addressed during our discussion on accountability . Turning now to the next bullet point which refers to informed decisions about the deployment and use of weapons , IHL clearly requires that those responsible for planning , deciding upon or executing attacks reach their decisions about deployment and use of weapon systems based on an assessment of all information available to them at the relevant time . We agree with the position put forward by ICRC that the commanders and planners will need to anticipate and assess in advance the lawfulness of all possible strikes by the laws and that this must include all reasonably foreseeable changes in circumstances . We consider this a normal part of military planning which requires anticipating possible changes to circumstances based on enemy actions or responses . Turning now to the bullet point which refers to the human operator being able to change the mode of operation including deactivation . Several Delegations in this GGE have put forward the idea that laws must be equipped with certain technical features for them to be compliant with IHL . We consider that certain features such as self - neutralization and self - deactivation may be necessary in some circumstances depending on the operational context and other control measures available such as the ability of a human operator to intervene , suspend or cancel an attack . We do not regard these features as being required by IHL in all circumstances . Thank you , Chair .",Western European and other States,Yes
MEXICO,15:44:40,2024-03-06,0:08:46,"Thank you very much , Chair . I am daring to speak first taking into account that I actually have to leave the room for a few moments later . So I would like to share with you my views on this particular issue . Now , this is an issue that the Mexican Delegation believes is very important because in order to effectively have effective regulations we have to really know what the risks are related incorporating autonomy into weapon systems . Now , Mexico looks at the issue of risks in a somewhat broader way than how you , Chair , reflected that in the list and I will try to move from a more general assessment to more specific comments . The first thing , of course , we would like to say once again that we have to look at the question of risks from a perspective of ethics . These are indeed machines that are taking decision on life and death . Autonomous weapon systems present serious ethical issues where there is a cognitive distance in space , time and understanding between the human decision of human beings and the use of arms and there is no moral barrier where there is a transfer from responsibility in decisions to machines . Second , and we are repeating something that we already said in our general comments , the risks have to be also looked at from a security point of view . Lethal autonomous weapon systems present the risks of asymmetric wars , disproportionate use of force , unintentional conflicts , escalation of conflicts , generates an arms race and also they can start interacting with other weapon systems including WMD , third point or third category of risks . These are related to the situation when laws are being developed , when they are not placed under meaningful human control so they are not in a position to comply with IHL or other existing legal standards due to their nature or the way they operate . In this aspect we believe that the following risks arise . The first , the risks of excluding human control , especially when it relates to technical requirements for the operation of lethal weapons systems through preprogrammed profiles , information on the environment through sensors analysis of the information being collected by sensors and applied through the existing or preexisting profiles . Mexico believes that such processes cannot replace human judgment required to implement the fundamental principles of IHL in specific contexts . Also , there are risks related to such contexts . IHL and their legal obligations under international law depend on decisions that are aware of the context and can adapt to changing circumstances . Autonomy represents risks where the specific context is not really known . Space and time are also not processed correctly when these weapons are used . Then also there are risks related to the predictability of the effects of laws related to the attempts of users to limit the way these machines are used in line with IHL for risks of intentionality . Laws can simply go beyond what humans are capable to process . There is a risk to presume their capacity to recognize the context and also to adapt in a predictable manner to new situations that were not planned during the design phase or during the evaluation phase . Number 5 , risks relate to the human machine team . When we appreciate the need for human participation in the critical functions of machines , algorithms are extremely powerful to process extremely high levels of information , but this does not mean that they will carry out the correct causal analysis . Now , it is always necessary for humans to carry out necessary qualitative assessments and to determine legality or lawfulness . It has to be all adapted to changing circumstances and ensure control over the parameters , environment , and supervision during the use of such weapons . Now , where we have this link between humans and machines , still risks exist . For example , the situations where information presented is too complex to be analyzed by human beings before taking a decision , for example , identifying the deployment and use of models generated by artificial intelligence and it is also important to carry out that these limitations are even exacerbated when we are not just talking about one weapon system but when we have a network of such systems that operate in real time . So we are talking about a whole network of machines operating and this creates even more complicated situations . So this link implies that the participation of human beings will end up being simply nominal . Another challenge here is the situation related to operations where we see critical or unpredictable situations related to the application of force where humans are not in a position to have enough situational information to react in an adequate manner and to react promptly and maybe it will be even impossible for them to do anything . Then finally , the challenge of discrimination related to gender , race or other considerations and this can be related to the source of data in the system but also through the operation of processes related to algorithms . And very briefly on required measures , Mexico believes that we must distinguish between risks that require measures to simply avoid risks , risk avoidance measures which we believe are prohibitions and regulations or it is related to the discussions that we were having yesterday afternoon and this morning and also mitigation measures that may apply in a practical manner to analyze how this links up to norms of IHL and also how this will be linked to any norms that will be developed by this GG . Thank you .",Latin American and Caribbean States,Yes
ISRAEL,15:53:43,2024-03-06,0:02:11,"Thank you , Chair . We would like to make a few quick remarks on the subject of risk mitigation with regard to the development and use of laws . Approaching this topic , our Delegation understands that the concerns and measures discussed in the context in this context were intended to go beyond legal obligations encompassing good practices and policy measures , technical standards and other suggestions deliberated within this forum . While some of the concerns and measures discussed here are certainly relevant to compliance with IHL rules , even if only as practical or secondary aspects rather than primary rules , the variety of measures and their wide spectrum of implementation may often extend beyond what is required to comply with legal obligations . We suggest taking this in mind when addressing the topic . Generally in Israel's view , risk assessments and mitigation measures in their most basic sense constitute relevant tools for addressing uncertainty associated with emerging technologies , particularly improving reliability and predictability . Therefore , measures to address these concerns should be integrated at various stages of the weapons system life cycle . These measures may include but are not limited to adequate testing , evaluation and training , safety mechanisms and comprehensive assessments including legal reviews in this context . We would also like to note the distinction between foreseeability by design and technical malfunctions as these are two separate issues . A system with a very high degree of foreseeability in its functioning may still be subject to technical malfunctions as is the case with every weapons system . Finally , we welcome a fact - based informed discussion to identify and address realistic risks that are associated with laws and the appropriate mitigation measures to address those risks . Thank you , Chair .",Western European and other States,Yes
FRANCE,15:56:32,2024-03-06,0:02:38,"Thank you , Chair . Chair , I would like to make a few comments of a general nature on risk mitigation and confidence building measures . Our discussions in this body focus more on the challenges posed by emerging technologies in lethal autonomous weapons systems . Of course , it is vital that we find a collective response to these challenges . However , we should not lose sight of the potential benefits of lethal systems which have some part of autonomy . There are many such benefits including to implement and respect the principles of IHL . This is a very important aspect as well as our work should not lead to stigmatization of technology unduly , technology which might offer many advantages to support decision making , for example , to protect from dangerous tasks and also to improve targeting precision . And benefits can be brought in to operations substantively so long as there are risk mitigation measures which are provided for from design to the use of such weapons systems . France calls upon high contracting parties to establish adapted risk mitigation measures including technical guarantees . States should thus commit themselves to implementing strict procedures for qualification , for verification , for evaluation and for validation throughout the whole life cycle of the system to ensure reliability . Also , technical guarantees and organizational guarantees should be established to prevent abusive use , malfunctioning , diversion and abandoning the human prerogative . To identify respective responsibilities clearly of all persons involved , states should clearly identify the responsibility chain of all those involved in decision making from design , development , qualification and use of a lethal autonomous or partial autonomous weapon system . Thank you .",Western European and other States,Yes
AUSTRIA,15:59:29,2024-03-06,0:04:12,"We would just have a short remark on the overall risk . I think it is a very good question to pose to identify the risks that are there because in the past we have talked a lot about risk mitigation measures without clearly identifying the risks . We very much welcome also a debate about this topic . At the same time when we look at the list , we see that some of them are classic risks but some of them as they are listed here also are more something that comes from the lack or not implementing one of the measures that are listed in other parts of this document of our discussion . So not doing what we see as the proper way forward in addressing the risk surrounding autonomous weapon systems is not a risk per se but this is just something of a side note . What we see I think what might be useful if we have this debate is to do a little bit of a grouping . What we see are like Mexico before us , a set of broader security risks that are also more broadly linked to our work here . There is an overall humanitarian concern as the risk of humans losing control of armed conflict while it is humans who will continue to bear the consequences of armed conflict . This has been mentioned in the past already through a number of Delegations also that there is a risk towards peace and security and a possible arms race . There is the risk of proliferation as to use of autonomous weapon systems by non-state actors seems a very high risk once the Pandora's box is open . There is also as Mexico has mentioned a risk from integrating AI in broader weapon systems but I think we have addressed this in our discussion when we have discussed AI as functions of other weapon systems . Closest to our work are probably those risks that relate to IHL implementation and those are , of course , unintended engagement , unpredictability , incidental loss of life , injuries to civilians and damage to civilian objects resulting from the use of weapon systems as such , loss of control of the system . And in this regard we also want to stress that the risks of unpredictability of a weapon systems caused by self learning and the black box problem would not only violate IHL but could impact the military utilities since no military has use for weapon systems it cannot control . There are specific security risks related to the weapon systems , cyber threats , spoofing , using other AI specific vulnerabilities and then there is this more broader AI risk list that has also already been mentioned that relates to , again , the black box problem , automation bias , discrimination for building bias and risks related to datasets , risks regarding the integrity , quality and veracity of data that is used . This also relates a little bit to the interaction between the military or the state side and the private sector . And then there is also a risk that stems from a lack of adequate training of personnel on all relevant levels and on a more broader scale also the skills degradation . These are just some general risks that we wanted to list under this topic . Many of those risks can be mitigated or even eliminated by human control and others , this is also clear , would mostly be mitigated through legal reviews , through measures on the national level as well . There is many risks but also many different ways on addressing them . In our view , the human control is already a very big aspect of this and does a lot of good work . Thank you .",Western European and other States,Yes
RUSSIAN FEDERATION,16:04:00,2024-03-06,0:09:09,"Thank you very much , Distinguished Chair . We also would like to share our comments and our thoughts on the issue of risks related to lethal autonomous weapon systems . First of all , some general comments on the theme of risks in general and how this issue is related to the mandate of our group . We believe that the discussion on risks related to autonomous weapon systems this discussion is not really directly related to the mandate of this group . It is just a first step towards the consideration of measures on the regulation or removal of risks connected to such weapon systems because our main objective as we understand it is to discuss potential measures that may be adopted to be applied to such systems . Moreover , the question of risks is something that we believe should be balanced when we take our approach to this issue . When we talk about risks , we should not forget that the use of lethal autonomous weapon systems in comparison to other weapons systems that are being employed during military conflicts , this use may represent some advantages and provide some opportunities in the context of compliance and implementation of the principles and norms of IHL . Specifically , we are convinced that lethal autonomous weapon systems can demonstrate greater levels of effectiveness than a human operator in solving the objectives that were set and reduce any errors . In particular , such systems are capable to seriously reduce the negative consequences of the use of weapons in the context of IHL related to operator errors , the psychological and physical condition of the human being , moral , religious and ethical considerations . Their employment in line with IHL may increase the precision of attacks against military objects and may contribute to reduce the probability of unintentional strikes against civilians and against civilian objects . We believe that all of these advantages together with the risks that we are discussing right now should be taken into consideration by our group as we discuss any specific measures that can be applied to lethal autonomous weapon systems . Now , on the question of risks , we would like to especially highlight such risks that directly relate to these weapon systems and do indeed represent a reason why we need to give additional consideration to them in the context of compliance with IHL . And the risks are as follows : The use of weapon systems with artificial intelligence or lethal autonomous weapon systems that these systems end up in the hands of non-state actors , including terrorist formations and some of the bullets on the screen are connected with this issue and they could be kept here . We welcome the fact that they are represented in this list . Another risk that requires our attention is the loss of control over a system as a result of a technical malfunction or break - in or the reprogramming of a system by malicious actors . And we see that some of the bullet points on the screen is related to this risk . And we are ready to work with these ideas because they do require that delegations give attention to this as we discuss and agree to specific measures related to mitigation of risks connected to laws . And as a third risk that we would like to highlight is something related to the taking by the system or by the operator of erroneous decisions . And when we refer to these three types of risks , we believe that through this type of language , we actually are covering all the bullet points that you see on the screen because the questions of human judgment , the questions related to the behavior of systems , as well as the notions of predictability , reliability , we believe that they all boil down to the risks that I referred to . And they relate to a situation where a system or the operator can take an erroneous decision . We believe that this is exactly what we should focus on as we discuss the different measures to regulate or remove such risks . And here we would like to stress and also join what other Delegations said before us , saying that the discussion of all the questions related to the mitigation of risks , all of this should be done with a direct connection or link to existing international humanitarian law and ensuring compliance under IHL , keeping in mind the certain technical features of lethal autonomous weapon systems . Thank you .",Eastern European States,Yes
PAKISTAN,16:13:33,2024-03-06,0:07:58,"Thank you , Chair . Chair , on the question of risk mitigation and confidence building , first we would briefly describe what our understanding of risk mitigation and confidence building measures is . We in our past work , we have always understood that these measures do not exist in isolation and specifically in this body in which we are working in CCW , whenever we have had discussions , we have always held this view that the purpose , the main purpose of this body is codification of existing law , progressive development of law , mainly IHL , the international law of armed conflict and in that context , the best way to mitigate the risks are to address directly the challenges in a way we have addressed them in the past . However , having said that , there is certainly value in the risk mitigation measures as long as they are linked with something , as long as they are complementary to something . They are not existing in silo or in a vacuum . If we look at the examples of past , as you have also pointed out , Chair , that we are not starting from scratch . We have to build upon our previous work . We see in the existing protocols also in protocol 5 , for instance , there is a technical annex which talks about best practices , which talks about different practices that need to be instituted at the state level . For instance , for greater compliance of Article 4 , Article 5 and Article 9 of the protocol 5 . So in that way it is very clear that those kind of best practices are rooted in that protocol . Second point would be that what my distinguished colleague from the Russian Federation has stated , I would very much agree with the characterization that while the work that we are doing under this topic is not directly linked with the mandate which the GGE has been tasked to do , at the same time it is very much the start at the outset , the commencement of our work because if we are going to be addressing the challenges , we need to be cognizant of what those challenges and risks are . So it is very pertinent that we are addressing this issue right at the outset during the first week . And we are also very pleased to see the first question that you have outlined where you are inviting the Delegates to share what are those possible risks in this area . Then next I would just like to endorse some of the points that have been made by Mexico , for instance , that when we are talking about the risks , I think it would be better that we go in a holistic , in a comprehensive manner and in a structured manner as well . We understand that the text , for instance , that is on the screen is not something that the Chair has himself proposed but is coming directly from the submissions that have been put forward by the state parties . I think going forward it would be valuable if we are able to structure our risks in a manner in which we have done in the past as well and in a manner in which they have been proposed and raised in this body . Here I would very much like to agree with our Distinguished Colleague from the US who also pointed out under other topic that if we are able to follow this kind of structured methodology under various topics , we would be able to perhaps narrow down our points of convergences . So when we talk about different categories of risks , we understand that some of them or most of them on the screen that have been presented are mostly a result of different omissions or different actions that the states , parties or users or operators , if they are unable to take those actions in terms of development , deployment or use of these weapon systems , what those risks could result in . At the same time , we would like to highlight that we would like to categorize these risks under what was described as humanitarian , ethical and security considerations . I think that would give us greater clarity . From what we see , many of the risks that are already on the screen could be parked in one of those categories right now . One category I think that we need to take stock of is the risks under the category of security considerations . This is very much pertinent to the work that we have been doing in the past as well . When the work started in the informal body of experts in 2014 and the report that we all adopted by consensus in 2016 , the group identified what those possible risks from the security angle would be which merited further study and consideration . I would just read them out here . The risk of an arms race , the effects on the threshold for armed conflicts , the effects on regional and global security and stability and proliferation risks including to and by non-state actors and risks posed by cyber operations in relation to laws . So this was what the group agreed by consensus in 2016 . Then going forward , we also saw that very comprehensive discussions took place in 2019 in that GGE as well in which there was broader understanding and convergence of these risks under security considerations and some of them we were able to allude to , refer to in our guiding principles as well which you have detailed in your opening remarks . In 2021 in the Chair's summary under an exclusive agenda item as well and then in 2023 when we had the discussions on the negotiation of the report , we had a separate section on risk mitigation . So these conversations have been taking place for a while . It is important that at this stage we are able to take stock of all of these conversations and proposals in a more holistic manner and park all of these risks which have been brought up by different high contracting parties under these three categories , humanitarian , ethical and security considerations . I would just like to briefly subscribe also to the views that have been expressed by Mexico , Austria as well relating to humanitarian category and ethical considerations as well , the risks that they have very eloquently elaborated upon . So we have going and based on this categorization we have comments to offer as well on what measures are needed for those risks but I believe that is a question that I think you would perhaps would like to address it under the second question when we go to that . Thank you . Thank you very much for that .",Asia-Pacific States,Yes
SWITZERLAND,16:21:39,2024-03-06,0:05:54,"Thank you , Mr. Chair . We really appreciate this discussion on risks and we broadly agree with the topics that you have listed . We are moving very fast and what we are going to contribute at this stage is just the first reaction . We might come back with more specific points later . I also want to say that it is hard to follow the Distinguished Colleague of Pakistan who spoke just before me because a number of the points that I am going to say he just made them very eloquently . Our overall understanding on the broader topic of risk mitigation confidence building measures is that in addition to measures that are strictly necessary to ensure compliance we also need additional measures , complementary . Our view is also that such measures must not be in the vacuum . They must not stand isolated . They must not stand instead of a clear regulation . One way to see this is that we perhaps would anticipate a further future legal instruments with prohibitions and regulations and then a complementary set of practices , measures , for instance , outlined in an annex to a protocol or to an instrument of whatever nature it is . We think we can now start thinking a little bit more clearly about how this could articulate the provisions , the legal provisions and then further measures . At this stage I would first like to pick up the discussion about the risks overall and like other speakers before me , try to talk a little bit about what we have here on the screen . Our first general point and here as Pakistan has just said we think that this list could be streamlined , structured , categorized . We have talked about risks in the past and we were able to line them up in a coherent fashion . Then a second very important point relates to what Austria and Mexico have said before us . Risks must be really better understood . There are many possible risks that we are just about to understand here . We might not yet have the full picture , even risks , there might even be risks that today are yet unknown . This is really a difficult discipline here for us because the technologies are emerging and so it will be very important that we start thinking about how we will be able to continuously improving our understanding of risks in a comprehensive manner . And that comprehensive manner , that brings me to another general point . It is important for us that we in this forum where we pursue an IHL focus stay aware of the fact that while IHL compliance is extremely important and noncompliance is , of course , an enormous risk as these weapons develop , there is also other risks and concerns . Risks that go beyond of what we are looking at usually in this forum . For instance , ethical aspects , they have been mentioned by others , but then also potential impact on international security , stability , for example , the potential possibility of an arms race , escalation risks , decrease in crisis stability , and then we have more technical risks , guiding principle F in 2019 already referred to physical security , hacking , data spoofing , et cetera . Then we have also talked a lot about the unintended biases related to gender , race , age , ability , et cetera . And we have also referred in the past to the risk of acquisition by terrorist groups , nonstate actors and the risk of proliferation . And to end this first intervention , I would just briefly remind Delegates of the preambular paragraph 4 of the UNGA resolution adopted last year which kind of made a good summary of the concerns that are out there , perhaps also out there beyond the CCW and which are on the top of the minds of 177 countries , I believe , and that is concerns about global security , regional and international stability , including the risks of an emerging arms race , lowering the threshold for conflict and proliferation , including to nonstate actors . So it is not to say that the risks that are on the screen here are not the right ones to list , but it is just to make sure we stay on top of things and have a broader risk perspective and that we aim to always try to better understand risks that are only about to emerge . Thank you .",Western European and other States,Yes
CUBA,16:27:49,2024-03-06,0:04:06,"Thank you , Chair . I would like to share with you some of our thoughts on the question of risk mitigation in this process . My Delegation believes that the focus that should be prevail in connection with laws is not simply to prevent or mitigate risks , but to avoid risks in the context of regulations and prohibitions . We believe in order to do that , we need to ensure that human control is maintained . We also have to consider physical and nonphysical barriers that would prevent the acquisition of these weapon systems in the hands of nonstate actors putting obstacles on the proliferation of such weapon systems . We believe that anything that uses software can be hacked or used by state and nonstate actors and laws also can fall victim to such actors . And due to the nature of such systems , they will be very vulnerable to cyber attacks . We have to stress that we cannot say that there is no possibility that when these weapon systems are used in the battlefield , they are software cannot be manipulated and also it may lead to a situation that an operation cannot be aborted if there are malfunctions to the system . And these are technical vulnerabilities of any machine , but in these , in this particular situation , it can be the great loss of human life , environmental losses and other harm . This strengthens our belief just like other Delegations on the need to ban completely laws , including those that do not include the or those that include the possibility for their deactivation . This is connected to serious concerns related to cybersecurity and as such systems are deployed and used , especially when these systems are not communicating with the operators , it leaves a lot of time for the systems to be manipulated by others . It can take time to identify security vulnerabilities in an autonomous machine and we have to ask a question , will it be possible to correct this vulnerability in a remote fashion or recover the device or ensure that there is human control at all of these stages ? Also , we believe that confidence building measures , even this is a useful tool that has been employed very successfully in other areas , will not be sufficient in this particular situation . When we talk about laws , in the context of this new mandate and the elaboration of a new instrument of other measures , we have to focus first and foremost on prohibitions and regulations to only after that complement this with other CBMs to promote transparency and ensure physical security of such system avoiding risks of diversion but by not state actors and others . Thank you .",Latin American and Caribbean States,Yes
PHILIPPINES,16:32:14,2024-03-06,0:01:43,"Thank you , Mr. Chair . The Philippine Delegation wishes to add some of the risks which were surfaced during the meeting on Indo - Pacific Perspectives on autonomous weapon systems by states , many of which are not represented here . In the interest of time , the Philippines would only articulate and highlight two . First is the impact of autonomous weapons on armed escalation of existing conflict . The benefits of autonomous systems in reducing human casualties may attempt military commanders to use them early in conflicts . This can lower the threshold for the use of force and escalate existing conflicts . Second and personally more important for me is the possible impact of the development of these weapons on the environment and ecological integrity , particularly in our case are their impact on the maritime space and environment . It has been frequently commented in various fora that autonomous submarine warfare is relatively more acceptable due to the low risk of human collateral damage underwater . But we wish to point out that damage to the marine ecology that these weapons may cause is directly related to the livelihood of many island and archipelagic states in the Indian and Pacific oceans . To many in our part of the world , marine ecology is life . We hope that these risks and concerns can be taken into consideration as well in our discussions . We would also like to support the comments made by Pakistan and Switzerland on the utility of identifying categories of risks such as humanitarian ethical and security considerations to better structure our discussions . Thank you , Mr. Chair .",Asia-Pacific States,Yes
URUGUAY,16:34:17,2024-03-06,0:01:38,"Thank you very much , Distinguished Chair . I would like to make a couple of brief comments during this interactive dialogue just like was done by Austria following certain statements made by colleagues . I would like to say on risk mitigation , I agree with the views of Mexico and Pakistan related to the risks to international security and my Delegation agrees with that . And also to recall that we are talking about an environment where IHL applies but not only international humanitarian law but also international law and different branches of law . And also to recall that the majority of states are not participating in mitigation risks exercises and they suffer from the effects of the use of different weapons and also we have to say there are some risks that have to be avoided or eliminated and not simply mitigated . I was interested in the comments made by Switzerland about the possibility to include an annex to the possible protocol with measures related to risk mitigation . So that will be it for the time being . Thank you .",Latin American and Caribbean States,Yes
INDIA,16:36:10,2024-03-06,0:00:00,"Thank you , Mr. President and thank you once again for the very informative visuals that you have up there on the screen . My Delegation would like to is under the impression that we are discussing possible elements of an instrument . That is the mandate of this group . We have been discussing risks for the last ten years as many of our colleagues and many Delegations here have reminded us and it is my understanding that most of what the risks of laws are clearly understood . My Delegation believes that we are focused on an instrument and would like to be guided by the place of enumerating risks if there is a place for that in a possible instrument . This section in an instrument , if such an instrument reaches a stage of further consideration , in our understanding addresses risks that cannot be addressed in the section on commitments and obligations . Several such risks that are listed here will be addressed by the obligations such as prohibitions and restrictions that are under discussion . There are also well established understandings based on the range of measures that are available when an instrument such as this is being constructed . The second of your guiding questions addresses that . Some of these are TCBMs , notifications , exchange of data , codes of conduct , et cetera . There has to be a relation between the risks and the mitigation measures that take into account , for example , applicable Treaties and international law . That is all , Mr. President . I just want to focus on the construction of an instrument in which the risks and mitigation measures are linked to each other and to the larger structure of an instrument instead of going on to a general discussion on what all possible risks associated with the use of laws are . Thank you .",Asia-Pacific States,Yes
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,16:39:55,2024-03-06,0:02:43,"Thank you very much , Chair . My general comments here want to focus on the question of using agreed language when identifying risks . Indeed Delegations have emphasized the importance of using agreed language . In our understanding agreed language does not only mean language included in previous reports of this group but also includes language found in IHL and other binding sources of international law . There are three phrases or terminologies currently Chair on the screen in reply to your prompts that we want to focus on . The first phrase is the phrase unintended engagements which does not appear in the text of the CCW or the Geneva Conventions in the additional protocol . Does the risk of unintended engagements mean the risk of unintended killing of civilians or protected persons ? If that is the case it is better to keep to IHL language on indiscriminate attacks or civilian harm . Second , phrases like algorithmic bias or simply bias do not sufficiently capture the risk being referred to . In its place it may be better , for example , to talk about the risk of discrimination or the risk of gender or racial discrimination , phrases which exist in binding legal norms . Third , the phrase risk mitigation , the phrase risk mitigation does also equally does not appear in the text of the CCW or the Geneva Conventions and the additional protocols . It is critical that we explain what we mean by this and how this aligns with established international legal obligations such as those that are found under international humanitarian law . Chair , some risks that are posed by laws already necessitate compliance with state obligations that exceed mere risk mitigation . Let me just give two examples , Chair . Let's talk about the risk of arbitrary deprivation of the right to life due to failure to comply with rules of distinction , for example . The obligation there is an obligation of prohibition on arbitrary deprivation of life . It is not an obligation to mitigate that particular risk . So when we say we are trying to mitigate risk , we need to be clear which particular risk we are referring to . If you look , for example , at the risk of racial and gender discrimination , the existing international obligations on state is to eliminate all forms of racial and gender discrimination . It is not to mitigate that particular risk . So it is very important in this particular regard for us to have some clarity . Lastly , it is evident , Chair , that certain risks posed by laws entail international obligations that surpass mere risk mitigation . While introducing new terms is permissible , caution must be exercised to avoid terminologies that undermine existing obligations . In cases where clear state obligations pertain to specific risks , any proposed risk mitigation should be regarded only as supplementary to existing obligations . With that , Chair , I thank you .",Non-applicable,No
GUATEMALA,16:44:07,2024-03-06,0:02:08,"Thank you very much , President . Since it is the first time I am taking the floor today , I would like to thank you for the work that we managed to accomplish here in this group . Indeed , this is a very enriching discussion . We have been seeing some convergence on some issues and we see that we have to discuss others . Now , some general comments here . Guatemala believes that we have to look at the issue of risk mitigation and confidence building measures in the area of laws . We have to , in this context , qualify the risks related to use of artificial intelligence and weapons systems to see what consequences this may lead to in terms of international peace and security . I would like to list some of the risks that we have identified supporting what was said by other Delegations before us . First , the possibilities that these weapons systems will fall into the hands of terrorists and non-state actors and situations where human judgment will not be applied and also the vulnerability of the infrastructure of the systems to cyber attacks and also who will be assigning the responsibility for the use of such systems . Finally , we would like to support the comments made by other Delegations on the need to eliminate risks that are incompatible with IHL and not simply limit ourselves to mitigation . Obviously , there are quite a few other risks that we must refer to , but this will be our views right now so that we can continue our conversations and I do hope that I will be able to take the floor once again to make some specific comments on the bullet points we see on the screen . Yes , of course .",Latin American and Caribbean States,Yes
EUROPEAN UNION,16:46:31,2024-03-06,0:00:56,"Thank you , Chair . I would like to make a general comment and one of more specific nature on risk mitigation . The European Union is of the view that tailored risk mitigation measures including those across the life cycle should be adopted and implemented and with regard to the question of risk with regard to the development and use of laws and specifically on bullet 4 , the EU recognizes the critical role that data plays for AI - based technologies . Social biases that have a potential impact on emerging technologies , for example , for gender bias in algorithms should also be given due consideration . Thank you .",Non-applicable,No
UNITED KINGDOM,16:47:42,2024-03-06,0:05:16,"Thank you , Mr. Chair . Over the years , this group has heard various categories of risks in relation to autonomous weapon systems . Like a number of previous Delegates have suggested , these risks really need to be streamlined and categorized . We would suggest that there are geopolitical and security risks including proliferation risks , ethical risks , application risks , that is those risks which apply in the context to the development and use of autonomous weapon systems and system security risks . The discussion leading to the conclusion of elements of an instrument will need to be based upon a good understanding of which category of risks those elements are addressing and how that risk engages . We note so far in this session that this group has already had some good discussion under your guidance which has both implicitly and explicitly raised risks such as absence of context appropriate human involvement , accountability , chains of command , limitations and predictability to name but a few . Therefore , the key matter here is how we mitigate those risks and introduce or articulate confidence building measures . We believe that those risk mitigations and confidence building measures would be key to the second tier of the two - tier approach with the nature or category of the risk absolutely material to the nature of the measure to address it . Looking at the risks on the list that you have provided , they therefore might be categorized as failure and reliability risks in use and design which would incorporate bullets 1 , 2 , 3 and 4 . Governance risks which would be bullet 6 . Geo and national security risks which would be bullet 7 and system security risks which would be bullet 8 . Bullets 5 and 9 are written in a degree of short hand and are probably captured elsewhere but would largely appear to sit in the failure bracket . But as I have suggested , the categories identifiable do not reflect a comprehensive selection of potential risk categories . That said , one overarching factor is that fast developing complex technologies need to be regulated in a manner that is tailored to the relevant technology concept and manner of use . This is a dynamic process which requires the development and sharing of expertise and practice that best enables the fundamental principles of the law to be enacted , making a tangible impact in the development and use of future weapon systems , helping to mitigate any risks associated with the integration of autonomous functions within weapon systems whilst maximizing the benefits . Both the UK proposal for an elaboration of a document on manual that would constitute an authoritative and comprehensive statement of the application of IHL and agreed best practice and the paper that the UK is cosponsoring would be useful and important outcomes that would build confidence that there are rigorous principles of concern that govern the use of autonomous weapon systems and ensure accountability in accordance with IHL . The UK's proposal is for the GGE to commission a document that sets out guidelines , advice and best practices on how states should approach the development and use of emerging technology in the area of laws at each stage of its life cycle . This would include an assessment on any characteristics which would be necessary compliance with IHL and any characteristics which would be incompatible with IHL . The document might include examples that illustrate what good practice is or might look like , providing a clear framework for the operationalization of the GGE guiding principles and actionable guidance for policy , technical and military stakeholders . It would thereby encourage the adoption of national regulations designed to strengthen respect for international humanitarian law and the UK paper outlines suggestions for areas that might be included in the document . We have previously stated that these areas would not be viewed as exhaustive or definitive but simply as an example of the sorts of things that such a document could cover . The cosponsored proposal sets out a series of regulatory measures on the basis of international humanitarian law . It advocates a series of draft articles which would constitute an impactful measure by setting forth common international understandings of prohibitions and other regulatory measures for the effective implementation of IHL principles and requirements in relation to laws . We believe that both these proposals provide a meaningful advancement to efforts to drive the behavior of states in relation to the adoption and use of autonomous weapon systems and would be important and useful risk mitigation and confidence building measures . Thank you .",Western European and other States,Yes
CHINA,16:57:45,2024-03-06,0:03:53,"Thank you , Chair . Our Delegation appreciates the contributions made by all parties on issues relating to laws which can lay a foundation for future discussions of the GGE . On risk mitigation and CBM , we have noticed the references on the screen on text about legal reviews , testing , evaluation , as well as other issues . Our Delegation would like to highlight the following observations . First , on testing and evaluation , bullet No. 18 , and training , bullet No. 28 , we should further strengthen testing and evaluation in light of technological development and avoid misuse and abuse of relevant technologies . We should also provide appropriate and targeted training so as to enhance the understanding of laws by commanders , operators , researchers , and other relevant personnel . Second , on legal review , bullet No. 25 , legal review is stipulated in Protocol 1 of the Geneva Conventions . All states parties should strictly implement their obligations . Most states parties , including China , have relevant review and evaluation mechanisms . As a CBM , countries are encouraged to formulate corresponding legal review mechanism in light of their national conditions . Third , on responsibility , No. 41 to 45 , the ultimate responsibility for the development , deployment , and use of weapon systems lies with humans . Countries should clearly define relevant responsibilities in accordance with their national policies and regulations and applicable international laws . At the same time , the due use nature of emerging law - related technology should be fully considered . An overly strict accountability system may have a negative impact on the development and application of emerging technologies . Fourth , on temporal / spatial restrictions , bullet No. 59 is not right now displayed on the screen . Consensus was reached during the previous session of the GGE . The final report states that , I quote , when necessary , limit the types of the targets that the system can engage . Limit the duration , geographical scope , and scale of the operation of the weapon system . Fifth , we encourage all countries to conduct voluntary exchanges and cooperation on risk mitigation measures , including through case studies , scenario analysis , experience sharing , and communication with international organizations and nongovernmental organizations in order to jointly promote the peaceful use of emerging technologies and bridge the technological gap among countries . We oppose the restriction of technological development of other countries on politicized grounds and the creation of obstacles to international cooperation exchanges in the field of new technologies . Thank you , Chair .",Asia-Pacific States,Yes
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,17:02:16,2024-03-06,0:01:47,"Thank you very much , Chair . It is just a really quick intervention on the utility of legal reviews as a mitigation measure . Chair , we acknowledge that actually legal reviews are important and are critical and can be able to serve this purpose . But I think there is an important point that we ought to remember . Legal reviews , in other words , they are conducted in accordance with existing international law , existing international humanitarian law and other relevant legal norms . Now , Chair , one of the key questions that is yet to be answered or for Delegations to find common ground is whether existing law , the law in actual effect that we use when we conduct legal reviews is adequate to govern lethal autonomous weapon systems . Now , ICRC and other notable stakeholders have noted that existing law may not be adequate to govern autonomous weapon systems . Now , in that regard , Chair , legal reviews utility in this particular point become very limited because we will be in a way using a measure or a scale that is inadequate to try and see whether , for example , a weapon is acceptable in terms of law . So that is a point which we just want to underline to say legal reviews utility can only be useful where existing law is adequate to govern the particular weapon that we are dealing with . Thank you for that remark . And , of course , one of the big questions here is indeed whether and how much additional regulation we need . So then in the end the problem could be solved . But yes , as we stand now , you might really have a point .",Non-applicable,No
ICRC,17:04:31,2024-03-06,0:03:36,"Thank you , Chair . The ICRC on this topic sees some of the things that are listed as mitigation measures here not as optional or context specific but rather as clear legal requirements and I think this was already flagged to some extent by some Delegations in the previous question including the representative from Mexico . I just wanted to highlight two very clear examples . Firstly , the reference to legal reviews , effective legal reviews are a clear obligation for states that are party to additional protocol 1 and for all states they are critical to ensuring that their armed forces comply with IHL rules in the use of any weapon systems . And the ICRC endorses the view which has already previously been recognized by this group's guiding principle in subprinciple E and previously by states in other sessions of this group . We endorse that weapons reviews of autonomous weapons are a crucial step in the process of ensuring compliance with international law just as they are with any new weapon system that is being studied or developed , acquired or adopted . The other clear example that I wanted to point to in this screen is the text in the sixth bullet point that refers to assessment of expected civilian harm and concrete and direct military advantage . Chair , this directly reflects the prohibition that is contained in Article 51.5B of additional protocol 1 and accepted as part of customary IHL and it prohibits attacks that may be expected to cause incidental loss of civilian life , injury to civilians , damage to civilian objects or a combination that would be excessive in relation to the concrete and direct military advantage anticipated . I think there are others in the list including regarding the training of personnel , allowing attribution of individual and state responsibility and instituting mechanisms for reporting , investigating and ensuring accountability for IHL violations . These are already clear existing IHL obligations . So we would have some serious concerns with any language that ultimately suggested that these obligations were somehow discretionary rather than mandatory . Finally , many of the other points on the screen relate to aspects that we have previously argued including throughout the course of this week need to be incorporated either as aspects of a prohibition on certain autonomous weapon systems and here we are particularly referring to the wording related to the predictability of an autonomous weapon or otherwise incorporated as restrictions on other autonomous weapon systems . And this is particularly relevant to the bullet on the second page that talks of limiting types of targets , duration , geographic scope and scale of use . We will not go into detail on these points here and instead we refer back to and rely on our previous interventions and recommendations but just to emphasize that in our view it is crucial that these prohibitions and restrictions be enshrined in legally binding international rules in order to provide certainty and clarity and also to address wider humanitarian risks and ethical concerns . No doubt these rules could be complemented and reinforced by common policy standards and good practice guidance but those kinds of measures can not be a substitute for clear binding new law . Thank you , Chair .",Non-applicable,No
AUSTRIA,17:08:21,2024-03-06,0:03:18,"Also we want to share some of our views on the list and on the proposed risk mitigation measures . I think also here we have to do a little bit of boxing , take the different measures that we are having here and think a little bit about where do they fit and what do we do with them . Many of them are quite interesting , quite useful with the big disclaimer and I think this is highly important and cannot be stressed often enough that the ICRC has just pointed . We cannot reinterpret or scale down existing international law . This is something we definitely should not do even if we have instruments where not all parties are part of but for most of the obligations that are widely adhered to , especially with the Geneva Conventions , it is extremely important that we do not do something like this . Then also another point that we want to reinforce is also the very valuable point by ICRC . If we go into something and I think this is a very popular risk mitigation measures and also an important one when it comes to implementation of IHL which are the legal reviews , you always have to have a strong set of legality that you test against . If we have the problem as the colleague has pointed out , that there are unclearities or gaps in the existing legal framework , then this has to be filled first . For these legal reviews to be fully effective . Coming back to the list and the different measures that we are seeing here , we also see several of them as part of the prohibitions and regulations that we have already discussed and that we want at least from our Austrian point of view see in a possible instrument . Then we are having some other measures that are quite useful as well that might either be and here we have different options , be part also of national level implementation , could be to some point a recommendation or and this is something we definitely need to discuss , we could also include them in a possible instrument as it has been done in protocol 5 . If colleagues have a look at this , there is a wide range of precautions or other measures that states could take that go broader than the original prohibition that is set out in the protocol . We could include this in the instrument or we could put it into another box . This is something to discuss . What is important for us in the first place is that we have a clear set of prohibitions and regulations first which have been indicated in your first questions and in our discussions also give priority to them . Everything here is mostly everything here is important and useful but it is not the priority in our work at the moment as a final point . Thank you . Thank you for that .",Western European and other States,Yes
UNITED STATES,17:11:46,2024-03-06,0:07:32,"Thank you , Mr. Chair . We are going to try to offer reactions on both the risk question and the risk mitigation measures question in this intervention and then we will return to the confidence building measures at a later time . Now on the risks themselves , we agree with many of the overall points that others including the UK and Switzerland have made today . The first is that many of these risks are subsidiary risks of broader and overarching risks on the list and the second is that we already have a significant body of consensus language on risks , including in the guiding principles and in prior GGE reports . For our Delegation , the most important risk to focus on and seek to mitigate is the risk of unintended engagements which is closely related to the final bullet on risk of harm to civilians . We consider many of the risks identified in the bullets as related to these two overarching risks and do not think the risk of unintended engagements should be merged with the loss of control over a system or the loss of human control as these are not the same concepts . To respond to the point that the Delegate from ICRAC just raised , the US Delegation considers the risk of unintended engagements to mean that the weapon system is engaging targets that the system operator did not intend to engage . So in that sense it is broader than just civilian harm . The loss of control over a system does not necessarily lead to unintended engagements and unintended engagements may be caused by a variety of factors including but not only through the loss of control over a system . Now , just briefly with respect to the point about using previously agreed language , we did want to reiterate the point made by others that we have consensus language on many of these risks . So just for example , with regard to the risk noted in the list as loss of human control in the first bullet , we would point to the 2019 report where we have several paragraphs that reflect this general concern such as guiding principle D which refers to a responsible chain of human command and control . Another option consistent with the paragraph that we agreed in our 2023 report in paragraph 21C could simply be to omit the word human and refer to a loss of control . With regard to the second bullet that begins with unpredictable system behavior , we think this bullet needs a little bit of clarification . We should focus on unpredictability that may be related to unintended engagements or risk to civilians . So again , this is a subsidiary to those two broader risks . Finally , with regard to the third bullet on reliability , this is not a risk as we see it . So we would remove that from the list . And then lastly on the issue of bias , we would recommend describing the risk as unintended bias in AI capabilities . We suggest to add the word unintended to be more specific about the type of bias that are problematic . Indeed , distinction is an important IHL requirement . We also think it is appropriate to refer to AI capabilities rather than just generally to AI . Now moving on to risk mitigation measures , this is a topic where the GGE has had many fruitful discussions in the past . In looking at the various proposals that Delegations have submitted since 2022 , there is really a great deal of overlap and similarity in identifying concrete actions that states should take to ensure and promote compliance with IHL . And we also again like the prior topic already have a body of consensus language that we can expand on . Many of the concrete measures on this list from rigorous testing and verification to weapons reviews to understandable human machine interfaces are included in the prior two prior joint proposals cosponsored by the United States and also in consensus GGE language including the 2019 and 2023 GGE reports . While not specifically framed as a risk mitigation measure , paragraph 22 of last year's GGE report states that when necessary , states should in derailia limit the types of targets that the system can engage and limit the duration , geographical scope and scale of the operation of the weapons system . Now , in addition to these general points , we did just want to make a few specific comments on a handful of the bullets . We will start by addressing bullets 9 and 10 together . The ninth bullet addresses the use of a weapon within a responsible chain of human command and control . Our draft article's proposal includes a number of measures that states should implement to ensure comprehensive accountability for the use of autonomous weapons systems including that states should only deploy laws within the state's general framework for implementation of IHL such as operation of such system within a responsible chain of human command and control . Thus , as we see it , the tenth bullet which says permit attribution of responsibility to individuals and states is not a separate risk mitigation measure but rather a goal or effect of the measure addressed previously . With respect to the 11th bullet which begins with the phrase alternative tools to ensure human control , again , we do not see these as measures but rather as a potential goal or effect of other measures . As our Delegation has said many times , human control and human supervision and intervention are not legal requirements . The very purpose of autonomy is to allow machines to carry out tasks that humans have previously performed . That said , measures such as controls and limits on targets or the incorporation of self - destruct or self - neutralization mechanisms can be risk mitigation measures to reduce the risk of unintended engagements . These measures can be implemented through mechanisms or software programming in the weapon system but they could also be included as fundamental attributes in the design of the weapon system such as the maximum amount of fuel available to the system which would affect its operating duration or its maximum geographic scope or by the number of munitions that a weapon system carries . Lastly , with respect to the 12th bullet which begins with ensure predictability , traceability and accountability and also mentions things like reliability and explainability , again , these are not measures but rather a goal or effect of other measures that can be adopted to effectively implement IHL . Our Delegation believes that we must give precise and implementable guidance to states on what measures they can take to mitigate the risk of unintended engagement in the use of laws . Ensuring predictability is not a measure but many of the other measures on the list from rigorous test and evaluation to validation and verification measures , these are measures that we think would help address the underlying concerns that Delegations who suggested this language have in mind . Thank you , Chair .",Western European and other States,Yes
BRAZIL,17:19:53,2024-03-06,0:03:34,"Thank you , Mr. Chair . I would like to use this occasion to introduce in a more thorough manner the proposal that we have introduced as a working paper to this session . It was circulated as WP1 . This is a proposal for the GG to consider the possible utility to its work of IEEE standard STD7007 2021 . It is our perception that this standard can be useful in many areas of our work including in terms of risk mitigation , confidence building , transparency and accountability . The IEEE , the institute of electrical and electronics engineers is an international professional association . It was formed in 1963 with over 400,000 members in over 190 countries . In 2016 the IEEE established its global initiative on ethics of autonomous and intelligent systems . This initiative led to the development of the first global ontological standard for ethically driven robotics and automation systems , the IEEE standard that I referred to earlier . The standard was developed by a working group chaired by Professor Edson Prestes from the Institute of the Federal University of Brazil . This standard is an ontological standard aimed at enabling the development of ethically driven robotics and automation systems and it offers ethical methodologies for the design , development and deployment of artificial intelligence . It is a core ontology . So it represents a midlevel set of formalizations and commitments that are platform independent . It is intended to fit between an upper top level of or foundational ontology and lower domain or application specific ontologies . It has a data modeling representation at the level of abstraction that is above specific database designs . The data that is generated according to this methodology can be exported , translated , queried and unified across independently developed systems and services . It contains a set of ontologies that represent norms and ethical principles , data privacy and protection , transparency and accountability and ethical violation management . In our view , the STD7007/2021 might prove useful both as a risk mitigation measure and as a step towards confidence building and accountability . It offers a common platform for progress on the discussion of core concepts such as control mechanisms , limits on targets , duration and geographical scope of operations . By addressing these topics through the standards framework , states can establish the common approach for evaluating the development and use of autonomous weapons systems . Another key advantage is the standards ability to preserve confidentiality and to protect industrial Intellectual Property . It might be useful as a reference at all levels . At the national level , it can serve as a standard for the development and assessment of autonomous weapons systems , technologies at all stages of its life cycle . At the regional and international level , it might serve as a common platform for the exchange of information , confidence building and transparency and accountability measures . Thank you very much , Mr. Chair . I would like to thank you for highlighting the main aspects of your working paper .",Latin American and Caribbean States,Yes
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,17:24:14,2024-03-06,0:02:02,"Thank you . Once again , thank you , Chair . My apologies for taking the floor again . I want to start by expressing appreciation to the US Delegation for their response . The first thing which we want to emphasize again is the question that consensus language in the UNGG reports is important but must not be inconsistent with existing IHL language or other obligations . In relation first to the aspect of unintended engagement which the Delegation has explained that this refers to the system engaging targets other than those authorized by the operator , does this mean then that the system has autonomy in critical functions ? Does this not fall under autonomous open systems that must be prohibited rather than mitigated ? In relation to the second point on unintended bias , what again is meant by this ? Chair , in terms of rule 88 of customary international humanitarian law , the words that are used is adverse distinction . In terms of international human rights law treaties to which Delegates or States are party , the term that is used is the right to non-discrimination . In our view , the risk here can be properly identified as risk to the right to non-discrimination . The word unintended is not helpful because gender and racial discrimination or any other form of discrimination is not only harmful because it is intended . It is harmful because it is discrimination . And as ICRC , we insist on using language that already exists in binding treaties . Thank you . Thank you for your contribution . We have been moving around among the different slides for this topic 3 . So if you have any need for another slide being shown , we stand ready to do so . I also understand that the text has only been shared during the day . So you might need some further reflection for that . I am ready to offer that and we could come back to topic number 3 tomorrow morning . I am in your hands for that . Because if there is no more appetite to continue discussion now , I think then we should not try to force that .",Non-applicable,No
REPUBLIC OF KOREA,17:27:27,2024-03-06,0:01:49,"Thank you , Chair . I did not want to prolong the discussions , but I wanted to make the following comments , knowing that this might be the last chance to speak today . A number of risks and risk reduction measures listed seem to be directly related to elements that would form an integral part of regulations that would be required to ensure the lawfulness of development and use of laws . As such , and as Austria and the United States propose , it would be useful to do some boxing work work and also differentiate between the essentials and subsidiary considerations and countermeasures . To this end , after we complete the initial round of discussion to add to and delete from the current list on Topic 3 , we could work on coming to a common or shared understanding , if not agreement , on the main categories of prohibitions and regulations first and then deal with how we may address other subsidiary yet important considerations such as ensuring safeguards against the data theft , alteration and proliferation at a later stage . And on a final note and before I close , we would support the addition of the word unbiased in front of bias , sorry , in front of unintended , unintended in front of the word bias because some intended bias is necessary to ensure the lawfulness of laws in order to obtain the intended effect . In other words , to ensure that the laws that we employ target the targets that we have preselected , some bias would be necessary in some cases . Thank you . Thank you for reminding us of that . Okay . I think it was a long day . And we had a long of very , very good discussions . You gave us some homework .",Asia-Pacific States,Yes
UNITED STATES,17:30:03,2024-03-06,0:06:19,We will certainly start doing homework on streamlining categorizing that might help also future discussions .,Western European and other States,Yes
AUSTRIA,17:36:48,2024-03-06,0:02:26,"I think everyone has deserved to go home at this moment but it is a very interesting topic and I think it is a concept that deserves a little bit more attention . The big question here is the question is there unintended bias or not ? It is a conceptual question , probably one we should agree . There are different ways of seeing it . We see it and this comes probably also from the kind of work that has been done in other international organizations on AI as something that is per se unintended . The problem if you talk about bias is that bias is a reflection of something that is already intrinsic in societies . If you talk about bias in our context , any kind of discrimination that is there to comply with IHL is not bias . It is something that has been deliberately programmed into the system to comply with IHL . In that case it is not bias because bias would more or less be an accident . We had this discussion today . There are different types of biases . There is the kind of bias that stems from faulty datasets or not programming but kind of the way the datasets are selected , implemented , maintained . Also the cognitive biases that exist in human existence or the way that the human brain works . I have here a list that includes stereotyping , bandwagon effect , priming , selective perception or confirmation bias . Those are just cognitive biases that you can see in everyday's world and that are then introduced into an algorithmic system and then you have something that has come up often today , real life prejudices which are racial bias , gender bias and so on . It is a very wide area . It is all in most cases faulty . It is not intentional and a bias that is there for having kind of a positive discrimination on the battlefield in our interpretation is not a bias but it is something that has been placed there to comply with IHL . Thank you . Thank you for that explanation . Yes , I think I can now try to look at our work for tomorrow . I see three topics , issues to be addressed . A possible continuation of this discussion tomorrow morning after having given it again some thought overnight . Then we are doing some work to revisit the discussions we had earlier , for instance , on definition , characteristics and applicability of IHL making especially with regard to applicability of IHL making use of already existing texts , agreed text from earlier meetings and I promised you to have a discussion on way forward again today we have had already some reflections from certain Delegations but I think it would be good to have a kind of Delegated moment in the day tomorrow for that . We will look a bit at how things go in the course of the morning and then we have , of course , still the Friday morning and for the Friday afternoon we have another agenda item scheduled , other matters , that means other matters related to the mandate and the agenda . And that is it for my side . I am looking if there are any announcements . No , there are not . Then I would like to adjourn the meeting and see you tomorrow morning at 10 o'clock . Thank you very much . Enjoy your evening .",Western European and other States,Yes
